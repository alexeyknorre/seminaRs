---
title: "Где в России больше всего потерянных без вести?"
author: "Алексей Кнорре"
output:
  html_document:
    df_print: paged
---

Одной из задач на прошедшем митапе было взять данные официальной статистики МВД РФ о пропавших без вести, заботливо подготовленные Александром Минюшкиным и [выложенные](https://www.kaggle.com/miniushkin/missing-people-in-russia) на Kaggle, ответить на два вопроса: во-первых, пропавших без вести становится больше или меньше? во-вторых, в каких регионах России пропавших без вести больше всего?
Сами по себе данные -- хрестоматийный пример того, как плохо государственные ведомства в России понимают, что такое открытые данные. Обычно такими данными остановятся оцифрованные в Excel ведомственные статистические отчёты, находящиеся очень далеко от идеи [чистых данных](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf). В случае данных о пропавших без вести и находящихся в розыске всё именно так: строки не соответствуют наблюдениям (рядом с субъектом РФ мы можем увидеть и федеральный округ, и всю страну целиком), а колонки -- переменным (и сильно не хватает кодбука). Но давайте по порядку.
Если посмотреть внутрь отчётов, собранных на Kaggle, можно заметить, что, во-первых, называются все они непонятно, во-вторых, там не каждый отчёт -- годовой, а есть отчёты за полугодия. Я перебрал всё ручками, оставил отчёты только за каждый год целиком и назвал их так, чтобы название было годом. Так стало гораздо понятнее и удобнее. Отобранные отчёты вместе с кодом доступны [в репозитории](https://github.com/alexeyknorre/seminaRs/tree/master/2018.3.30/missing-people-in-russia/data).
Таким образом, у нас есть 4 CSV-файла за 2014-2017 годы с одинаковой структурой. Давайте посмотрим внутрь одного из них:

```{r}
df <- read.csv("missing-people-in-russia/data/2017.csv", encoding = "UTF-8", stringsAsFactors = F)
head(df)
```

Данные нужно сильно чистить: убирать вторую строку, выкидывать лишние переменные, переименовывать переменные, оставить только те значения, которые нам интересны (а скрывшиеся от органов дознания нам не интересны), кроме того, наши данные в так называемом "длинном" формате, когда для одной единицы наблюдения (в данном случае региона) переменные (разные типы потерянных людей) группируются в колонку "Наименование статистического показателя", а значения -- в колонку "Значение статистического показателя". Обязательно прочитайте подробнее про длинные и широкие данные, например, [здесь](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/).
Поскольку всё это нужно делать не один раз, а целых четыре (потому что четыре файла), все эти операции я оформляю в функцию.
Но сначала подгрузим библиотеки, которые понадобятся в дальнейшем:
```{r message=F, warning=F }
library(ggplot2)
library(dplyr)
library(rvest)
library(stringdist)
```

Итак, функция для чтения и подготовки данных. На вход функции подаётся число (год), которое мы используем как имя файла с данными и одновременно как значение для переменной года, когда будем объединять все файлы. Помимо технических задач обработки файлов есть и содержательная. В данном случае, что мы можем считать интересующим нас количеством? Просто количество пропавших без вести в отчётном году? Мне показалось, что это слишком наивно, и поэтому я вычитал количество человек, которые были найдены, что оставляло число тех, кто не был найден.
```{r}
read_data <- function(year){
  # создаём переменную пути, по которой находим файл с данными
  path <- paste0("missing-people-in-russia/data/",year,".csv")
  # прочитываем данные
  df <- read.csv(path,
                 encoding = "UTF-8", stringsAsFactors = F)
  # в прочитанных данных удаляем первую строку и второй столбец
  df <- df[-1,c(1,3,4)]
  # переименовываем столбцы
  names(df) <- c("region", "variable", "value")
  # оставляем только те наблюдения, которые содержат в колонке variable такие значения:
  df <- df[df$variable == "Всего разыскивалось лиц, в том числе лиц, пропавших без вести" |
             df$variable == "Установлено лиц из числа находившихся в розыске, в том числе лиц, пропавших без вести",]
  # говорит R, что переменная value -- числовая
  df$value <- as.numeric(df$value)
  # меняем формат данных с длинного на широкий
  df <- reshape(df, idvar = "region", timevar = "variable", direction = "wide")
  # Теперь можем посчитать разницу между количеством пропавших без вести за год и количеством найденных пропавших без вести
  df$lost <- df$`value.Всего разыскивалось лиц, в том числе лиц, пропавших без вести` - df$`value.Установлено лиц из числа находившихся в розыске, в том числе лиц, пропавших без вести`
  # оставляем интересующие нас колонки
  df <- df[,c(1,4)]
  # добавляем переменную года
  df$year <- year
  # говорим функции, чтобы по исполнении вернула датафрейм с обработанными данными.
  return(df)
}
```

Итак, наша функция сама открывает файл, чистит его и отдаёт готовый датафрейм. Красота! Теперь нужно применить эту функцию к 4 файлам, и при этом все четыре результата выполнения функции нужно склеить друг с другом построчно: 2014 год склеить с 2015, и с 2016, и с 2017... 
Для этого сначала создаём пустой датафрейм с тремя колонками и нулём наблюдений. Потом используем функцию `rbindlist()` из пакета `data.table`  (которая переводится как "скрепить построчно список из датафреймов").  

```{r}

df <- data.frame(matrix(nrow = 0, ncol = 3))
df <- data.table::rbindlist(list(read_data("2017"),
                                 read_data("2016"),
                                 read_data("2015"),
                                 read_data("2014")))

# Немного подчищаем: убираем федеральные округа, страну целиком, регионы с включенными автономными округами и непонятно откуда взявшееся ГУВД по СПб и ЛО:
df <- df[!grepl("округ|Всего|ФО|с а/о|с АО|ГУВД", df$region),]
df
```
Отлично. Теперь можно посмотреть на тренд:
```{r}
# Используем синтаксис dplyr, чтобы сделать агрегированную статистику по годам:
df_years <- df %>% 
  group_by(year) %>% 
  summarise(sum = sum(lost))

# Визуализируем:
ggplot(df_years, aes(year, sum)) +
  geom_bar(stat="identity") +
  xlab("Год") + ylab("Пропавшие без вести в розыске")
```

### А чё по регионам?

Интереснее, конечно, посмотреть, в каком регионе больше всего людей пропадает без вести. В абсолютных числах ответ понятен, тут к гадалке не ходи -- Москва и Московская область как самые густонаселенные регионы страны. Но вот если посмотреть на относительно количества населения, то есть сколько человек теряется в год на 100 000 человек по регионам?

Для этого используем рутину для выгрузки данных по населению из Википедии еше с самого первого занятия:
```{r}
wiki_url <- "https://ru.wikipedia.org/wiki/%D0%A1%D1%83%D0%B1%D1%8A%D0%B5%D0%BA%D1%82%D1%8B_%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D0%B9%D1%81%D0%BA%D0%BE%D0%B9_%D0%A4%D0%B5%D0%B4%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8"
wiki_xpath <- '//*[@id="mw-content-text"]/div/table[4]'
wiki_table <- wiki_url %>% read_html() %>% html_node(xpath = wiki_xpath) %>% html_table()
wiki_table <- wiki_table[,c(2,5,6)]
names(wiki_table) <- c("region", "area", "population")
wiki_table <- wiki_table[complete.cases(wiki_table),]
# Делаем нормальный столбец с населением
wiki_table$population <- as.numeric(gsub("[^0-9\\.]", "", wiki_table$population))
# Удаляем строку с Российской Федерацией
wiki_table <- wiki_table[!grepl("Российская Федерация", wiki_table$region),]
wiki_table$region <- gsub("\\[|\\]|[0-9]", "", wiki_table$region)
```

Теперь мёрджим (по-русски - объединяем) данные по потерянным без вести и по населению, причём делаем это с помощью fuzzy string matching:
```{r}
df$region <- gsub("Республика", "", df$region)
df$wiki_region_match <- NA

for (i in 1:nrow(df)){
  dists <- stringdist(df$region[i], wiki_table$region)
  names(dists) <- wiki_table$region
  df$wiki_region_match[i] <- names(dists[order(dists)][1])
}

df <- merge(df, wiki_table, by.x = "wiki_region_match", by.y = "region")
```

Готово. Теперь убираем лишние переменные, оставляем только 2017 год, делим количество потерянных на население и умножаем на 100 000, чтобы получить показатель на 100 000 человек, и, наконец, сортируем по убыванию. Интересно получается, не так ли?
```{r}
df <- subset(df, select = -c(wiki_region_match, area) )
df <- subset(df, year == 2017)
df$lost_rate <- df$lost / df$population * 100000
df <- df[with(df, order(-lost_rate)), ]
df
```


